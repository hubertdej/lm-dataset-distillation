{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUEGSi9HhA0F",
        "outputId": "e31ae481-8ccf-4157-d5e6-5ee8a749cabc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "sys.path.append(\"/content/drive/MyDrive/Ethz/CSNLP/csnlp_project/csnlp-dataset-distillation-main\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acutZXCVCFR",
        "outputId": "eb0bb833-044b-4c5f-caef-e5eb5b6adc1d"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_Zxkbw3ldhf"
      },
      "outputs": [],
      "source": [
        "from distillation_trainer import DistillationTrainer\n",
        "from state import State\n",
        "from data import Datapoint, get_wiki_dataloader\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, GPT2Config, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from copy import deepcopy\n",
        "import math\n",
        "\n",
        "# from VAE import VAE\n",
        "from torch.utils.data import DataLoader\n",
        "import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from networks.language_model import LanguageModelWrapper\n",
        "\n",
        "# print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXz5VsyZsC7z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxoe3isgL4Ev",
        "outputId": "1d1024f1-ed16-49a0-d483-d46fb80e2d6e"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "config = GPT2Config(vocab_size=50257, n_positions=1024, n_ctx=1024, n_embd=128, n_layer=4, n_head=4)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
        "print(f\"device is {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPYfIEiOOEEr"
      },
      "outputs": [],
      "source": [
        "block_size = 64  # tokens in each generated sentence\n",
        "min_text_length = 128\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "dataset_train = dataset[\"train\"]\n",
        "texts = []\n",
        "for data_point in dataset_train:\n",
        "    if len(data_point[\"text\"]) > min_text_length:\n",
        "        texts.append(data_point[\"text\"])\n",
        "    # if len(texts) >= L:\n",
        "    #     break\n",
        "# dataset_nonempty = [dataset_train[i] for i in range(len(dataset_train)) if len(dataset_train[i]['text']) > 200]\n",
        "# k = len(dataset_nonempty)\n",
        "# texts = [dataset_nonempty[i]['text'] for i in range(k)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h3Ah5fhr47B",
        "outputId": "66767904-a5d6-40f4-e32c-658252a233c7"
      },
      "outputs": [],
      "source": [
        "fraction = 0.005\n",
        "L1 = int(2.0 * fraction * len(texts))\n",
        "print(L1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH06Lh0cxUGG"
      },
      "outputs": [],
      "source": [
        "total_len = len(texts)\n",
        "LEN_sample = L1 // 2\n",
        "np.random.seed(42)\n",
        "idxs = np.random.choice(range(total_len), LEN_sample, replace=False)\n",
        "\n",
        "samples_sentences = [texts[idx] for idx in idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7RZzzHroLA0"
      },
      "outputs": [],
      "source": [
        "cropped_tokens = []\n",
        "\n",
        "for text in samples_sentences:\n",
        "    tokens = tokenizer(text)[\"input_ids\"]\n",
        "    cropped_tokens += [tokens[i : i + block_size] for i in range(0, len(tokens), block_size)]\n",
        "    if len(cropped_tokens[-1]) < block_size:\n",
        "        cropped_tokens.pop(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYMOkQsDqegz",
        "outputId": "72ccfe5a-79c1-403d-d8b1-7dee599ff41f"
      },
      "outputs": [],
      "source": [
        "L2 = len(cropped_tokens)\n",
        "print(L2, fraction * len(texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXxVjr8ZpZLA"
      },
      "source": [
        "We should generate at least 1% -> around 14k sentences, i.e. ~219 batches of 64 sentences.\n",
        "\n",
        "Ideally we should be able to reach 10% -> around 140k sentences i.e. ~2k batches\n",
        "\n",
        "Note: the dataloader we pass to train the sentences should have at least twice the number of sentences we will generate. Take into account that an original sentence is around 2 times the number of blocks it has. So for 1% we would need 14k original sentences for each update. That takes around 17minutes per generated batch. i.e. 62h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jZI1TjqOE_1"
      },
      "outputs": [],
      "source": [
        "seq_len = block_size\n",
        "batch_size = 16\n",
        "train_loader = get_wiki_dataloader(texts[:L1], tokenizer, block_size, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKbsZZG2wnvu"
      },
      "outputs": [],
      "source": [
        "samples_sentences = cropped_tokens\n",
        "extra = len(samples_sentences) % 64\n",
        "samples_dataloader = torch.tensor(samples_sentences)[:-extra].reshape(-1, 64, 64)\n",
        "# samples_dataloader = DataLoader(samples_sentences, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c-x90jiOLKu"
      },
      "outputs": [],
      "source": [
        "state = State(\n",
        "    device=device,\n",
        "    distill_steps=1,\n",
        "    distill_epochs=1,\n",
        "    batch_size=batch_size,\n",
        "    seq_len=seq_len,\n",
        "    distill_lr=3e-4,\n",
        "    lr=1e-2,\n",
        "    vocab_size=len(tokenizer),\n",
        "    decay_epochs=2,\n",
        "    decay_factor=0.1,\n",
        "    epochs=1,\n",
        "    checkpoint_interval=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7q9g4AqOzbZ",
        "outputId": "2ba0f60a-f661-4998-fa64-8fd760a0584c"
      },
      "outputs": [],
      "source": [
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n",
        "model = LanguageModelWrapper(gpt2_model, state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MliHSsYPLHEX",
        "outputId": "30cd889a-6ef5-45c3-ab88-aeb55341d864"
      },
      "outputs": [],
      "source": [
        "trainer = DistillationTrainer(state, model, tokenizer, train_loader, sampled_sentences=samples_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REgMukjub5o5",
        "outputId": "46ed9b08-54d2-4124-a854-27451b775e92"
      },
      "outputs": [],
      "source": [
        "trainer.iterative_generation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check how is the last generated sentence going"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resulting_text, token_ids, labels = trainer.get_train_data_and_text()\n",
        "print(tokenizer.decode(samples_sentences[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"generated_text_backup\", \"rb\") as fp:\n",
        "    gen_text = pickle.load(fp)\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Original sentence:\", tokenizer.decode(samples_sentences[i]))\n",
        "    print(\"Generated sentence: \", gen_text[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYPJmqdqNJdi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"generated_data\", \"rb\") as fp:\n",
        "    gen_data = pickle.load(fp)\n",
        "\n",
        "print(len(gen_data), len(samples_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(training_dataset, test_dataset, model=None):\n",
        "    model_checkpoint = \"distilgpt2\"\n",
        "    config = AutoConfig.from_pretrained(model_checkpoint)\n",
        "    eval_model = model if model is not None else AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        \"output\",\n",
        "        evaluation_strategy=\"no\",\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        num_train_epochs=1,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    eval_trainer = Trainer(\n",
        "        model=eval_model,\n",
        "        args=training_args,\n",
        "        train_dataset=training_dataset,\n",
        "    )\n",
        "\n",
        "    eval_trainer.train()\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        \"output\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        num_train_epochs=1,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    eval_trainer = Trainer(\n",
        "        model=eval_model,\n",
        "        args=training_args,\n",
        "        train_dataset=None,\n",
        "        eval_dataset=test_dataset,\n",
        "    )\n",
        "\n",
        "    eval_results = eval_trainer.evaluate()\n",
        "    print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_eval_dataset(evaluation_dataset):\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"])\n",
        "\n",
        "    def group_texts(examples):\n",
        "        # Concatenate all texts.\n",
        "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "        # Split by chunks of max_len.\n",
        "        result = {\n",
        "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "\n",
        "    evaluation_dataset = evaluation_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "\n",
        "    evaluation_dataset = evaluation_dataset.map(\n",
        "        group_texts,\n",
        "        batched=True,\n",
        "        batch_size=1000,\n",
        "        num_proc=4,\n",
        "    )\n",
        "\n",
        "    return evaluation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for fraction in [0.1, 0.2, 0.4, 0.8, 1]:\n",
        "    new_length = int(len(gen_data) * fraction)\n",
        "\n",
        "    synthetic_dataset = datasets.Dataset.from_dict(\n",
        "        {\"input_ids\": gen_data[:new_length], \"labels\": gen_data[:new_length]}\n",
        "    )\n",
        "    sampled_dataset = datasets.Dataset.from_dict(\n",
        "        {\"input_ids\": samples_sentences[:new_length], \"labels\": samples_sentences[:new_length]}\n",
        "    )\n",
        "\n",
        "    eval_L = 10000\n",
        "    evaluation_dataset = datasets.Dataset.from_dict({\"text\": texts[:eval_L]})\n",
        "    evaluation_dataset = update_eval_dataset(evaluation_dataset)\n",
        "\n",
        "    print(f\"Total fraction is {round(fraction * 0.05, 4)} %\")\n",
        "\n",
        "    evaluate(synthetic_dataset, evaluation_dataset)\n",
        "    evaluate(sampled_dataset, evaluation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        input_ids = input_ids.reshape(-1, input_ids.shape[-1])\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        logits = self.fc(lstm_out)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "        return {\"loss\": loss, \"logits\": logits}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm_model = LSTMModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for fraction in [0.1, 0.2, 0.4, 0.8, 1]:\n",
        "    new_length = int(len(gen_data) * fraction)\n",
        "\n",
        "    synthetic_dataset = datasets.Dataset.from_dict(\n",
        "        {\"input_ids\": gen_data[:new_length], \"labels\": gen_data[:new_length]}\n",
        "    )\n",
        "    sampled_dataset = datasets.Dataset.from_dict(\n",
        "        {\"input_ids\": samples_sentences[:new_length], \"labels\": samples_sentences[:new_length]}\n",
        "    )\n",
        "\n",
        "    eval_L = 10000\n",
        "    evaluation_dataset = datasets.Dataset.from_dict({\"text\": texts[:eval_L]})\n",
        "    evaluation_dataset = update_eval_dataset(evaluation_dataset)\n",
        "\n",
        "    print(f\"Total fraction is {round(fraction * 0.05, 4)} %\")\n",
        "\n",
        "    evaluate(synthetic_dataset, evaluation_dataset, lstm_model)\n",
        "    evaluate(sampled_dataset, evaluation_dataset, lstm_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
